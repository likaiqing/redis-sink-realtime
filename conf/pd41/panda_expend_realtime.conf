# A single-node Flume configuration for HDFS
# Name the components on this agent
# @version:    160505:3
a1.sources  = r1
a1.sinks    = k1
a1.channels = c1


a1.sources.r1.type                  = com.lxw1234.flume17.TaildirSource
a1.sources.r1.positionFile          = /data/logs/flume/expend_realtime.json
a1.sources.r1.filegroups            = f2 f3
a1.sources.r1.filegroups.f2         = /data/logs/current/charge/.*/.*expendnew.log
a1.sources.r1.filegroups.f3         = /data/logs/current/charge/.*/.*charge.expendip.log
a1.sources.r1.batchSize             = 100
a1.sources.r1.backoffSleepIncrement = 10
a1.sources.r1.maxBackoffSleep       = 50
a1.sources.r1.skipToEnd       = false

a1.sources.r1.interceptors                           = i2

# match interceptor (match target field)
a1.sources.r1.interceptors.i2.type                   = REGEX_EXTRACTOR_MULTI
a1.sources.r1.interceptors.i2.regex                  =^(.*?)    (.*?)   (.*?)   (.*?)   (.*?)   (.*?)   (.*?)   (.*?)   (.*?)   (.*?)   (.*?)   (.*?)   (.*?)$
a1.sources.r1.interceptors.i2.serializers            = s1 s2 s3 s4 s5 s6 s7 s8 s9 s10 s11 s12 s13 s14
a1.sources.r1.interceptors.i2.serializers.split      =\\t
a1.sources.r1.interceptors.i2.serializers.s1.name    = se_id
#app:平台
a1.sources.r1.interceptors.i2.serializers.s2.name    = app
a1.sources.r1.interceptors.i2.serializers.s3.name    = rid
a1.sources.r1.interceptors.i2.serializers.s4.name    = ce_id
a1.sources.r1.interceptors.i2.serializers.s5.name    = name
a1.sources.r1.interceptors.i2.serializers.s6.name    = quantity
a1.sources.r1.interceptors.i2.serializers.s7.name    = price
a1.sources.r1.interceptors.i2.serializers.s8.name    = total
a1.sources.r1.interceptors.i2.serializers.s9.name    = old
a1.sources.r1.interceptors.i2.serializers.s10.name    = new
a1.sources.r1.interceptors.i2.serializers.s11.name    = multi
a1.sources.r1.interceptors.i2.serializers.s12.name    = sign
a1.sources.r1.interceptors.i2.serializers.multi.new   = time par_date minute_field
a1.sources.r1.interceptors.i2.serializers.multi.par_date.type =org.apache.flume.interceptor.RegexExtractorInterceptorDateSerializer
a1.sources.r1.interceptors.i2.serializers.multi.par_date.pattern = yyyyMMdd
a1.sources.r1.interceptors.i2.serializers.multi.par_date.datetype =yyyy-MM-dd HH:mm:ss
a1.sources.r1.interceptors.i2.serializers.multi.minute_field.type =org.apache.flume.interceptor.RegexExtractorInterceptorDateSerializer
a1.sources.r1.interceptors.i2.serializers.multi.minute_field.pattern = yyyyMMddHHmm
a1.sources.r1.interceptors.i2.serializers.multi.minute_field.datetype =yyyy-MM-dd HH:mm:ss
a1.sources.r1.interceptors.i2.serializers.s13.name    = anchor_id
a1.sources.r1.interceptors.i2.serializers.s14.name    = ip



a1.sinks.k1.type             =com.pandatv.redis.sink.realtime.RedisSink

a1.sinks.k1.batchSize =100

a1.sinks.k1.redis=com.pandatv.redis.sink.realtime.RedisRealtimeExpendSinkSerializer
a1.sinks.k1.redis.host=r-2zeb2ddcf195e124.redis.rds.aliyuncs.com
a1.sinks.k1.redis.port=6379
a1.sinks.k1.redis.pwd=1gSUv6YVxg862EH3
#a1.sinks.k1.redis.host=10.120.3.74
#a1.sinks.k1.redis.port=6381
#a1.sinks.k1.redis.pwd=

a1.sinks.k1.serializer.condition=${app}!~star

a1.sinks.k1.serializer.mysqlUrl=jdbc:mysql://10.110.18.77:3306/villa?useUnicode=true&characterEncoding=utf-8&useSSL=false&autoReconnect=true
a1.sinks.k1.serializer.mysqlUser=analysis1
a1.sinks.k1.serializer.mysqlPass=Kdg0XFM6ixh70jZWC

a1.sinks.k1.serializer.sadd=true
a1.sinks.k1.serializer.saddExpire=600
a1.sinks.k1.serializer.saddKeyPrefix=rt_expend
a1.sinks.k1.serializer.saddKeyPreVar=${minute_field}
a1.sinks.k1.serializer.saddKeyName=total base64.encode{name} ${app} ${app}-base64.encode{name} $[anchor_id] $[anchor_id]-base64.encode{name}
a1.sinks.k1.serializer.saddKeySuffix=uv uv plat_uv plat_gift_uv classi_uv classi_gift_uv
a1.sinks.k1.serializer.saddValue=${rid} ${rid} ${rid} ${rid} ${rid} ${rid}
#每写一次key，需要修改哈希key对应的field
a1.sinks.k1.serializer.saddCascadHset=true
a1.sinks.k1.serializer.saddHashKeyPreVar=${par_date}
#saddHashKeyPrefix,saddKeyName,saddHashKeySuffix也与本身key中保持一致
#每一批次都cascade,要求不丢数据
#a1.sinks.k1.serializer.saddCascadHsetTime=90000
a1.sinks.k1.serializer.saddClassificationCascad=true
#每一批次通过rid记录UV,查询包含的uid对应的版区,此版区下的所有房间的uv union
#平台app,在原来基础之上修改


a1.sinks.k1.serializer.hincrby=true
a1.sinks.k1.serializer.hincrbyKeyPrefix=rt_expend
a1.sinks.k1.serializer.hincrbyKeyPreVar=${par_date}
a1.sinks.k1.serializer.hincrbyKeyName=total total base64.encode{name} base64.encode{name} ${app} ${app} ${app}-base64.encode{name} ${app}-base64.encode{name} $[anchor_id] $[anchor_id] $[anchor_id]-base64.encode{name} $[anchor_id]-base64.encode{name}
a1.sinks.k1.serializer.hincrbyKeySuffix=pv coin pv coin plat_pv plat_coin plat_gift_pv plat_gift_coin classi_pv classi_coin classi_gift_pv classi_gift_coin
#hincrbyField,hincrbyValue对应hincrbyKeySuffix
a1.sinks.k1.serializer.hincrbyField=${minute_field} ${minute_field} ${minute_field} ${minute_field} ${minute_field} ${minute_field} ${minute_field} ${minute_field} ${minute_field} ${minute_field} ${minute_field} ${minute_field}
a1.sinks.k1.serializer.hincrbyValue=1 ${total} 1 ${total} 1 ${total} 1 ${total} 1 ${total} 1 ${total}


a1.sinks.k1.serializer.platMap=gift_android:android;gift_android_hd:android;shop_android:android;gift_android_tv:android;gift_ios:ios;shop_ios:ios;gift_ipad:ios;gift_h5:m;gift_lobby:pc;gift_uwp:pc;gift_web:pc;shop_web:pc;star_android:star;star_android_finger:star;star_android_gift:star;star_android_hero:star;star_ios:star;star_ios_finger:star;star_ios_gift:star;star_ios_hero:star;star_ios_mall:star;star_pc_web:star;star_web_gift:star;star_web_hero:star;star_web_mall:star


a1.sinks.k1.serializer.hincrbyClassificationCascad=true

a1.sinks.k1.serializer.mysqlTime=45000

# Use a channel which buffers events in memory
a1.channels.c1.type                = memory
a1.channels.c1.capacity            = 100
a1.channels.c1.transactionCapacity = 100


# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel    = c1
